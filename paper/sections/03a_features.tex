\subsection{Features}
\label{sec:impl_features}

\begin{savenotes}


In Table~\ref{tab:featureTable} we describe all the features we used along with their respective individual feature count.


\begin{table}[ht!]
    \begin{center}
    \begin{tabular}{p{2.6cm}|p{6cm}|p{1.2cm}|p{1.2cm}}
    Feature                 & Description                                                               & Count             & Source\\ \hline \hline
    Blank line               & 1 divided by number of blank lines in the text                            & 1                 & \cite{de2001mining}\\ \hline
    Capital letter           & Capital letters divided by all letters                                    & 1                 & \cite{argamon2003style} \cite{de2001mining}\\ \hline
    Emoticon                & Boolean value for occurrence of known emoticons                           & 9                 & original\\ \hline
    Function word            & Fraction of words that are a known function word.                         & 280[DE] 280[EN]   & \cite{argamon2003style} \cite{de2001mining} \cite{madigan2005author} \cite{narayanan2012feasibility}\\ \hline
    Abbreviations           & Words that are a known abbreviation                                       & 53[DE] 53[EN]     & original\\ \hline
    Number character         & Fraction of characters that are numeric characters (0-9)                  & 1                 & \cite{narayanan2012feasibility}\\ \hline
    Paragraph               & Number of paragraphs divided by average paragraph length                  & 1                 & \cite{argamon2003style}\\ \hline
    PoSTag                  & Fraction of words that have a certain part-of-speech tag. To identify the part-of-speech tags we used the Standford Tagger\footnote{\url{http://nlp.stanford.edu/software/tagger.shtml}}
                                                                                                        & 55[DE] 46[EN]     & \cite{madigan2005author}\\ \hline
    Post length              & 1 divided by the number of characters                                     & 1                 & \cite{narayanan2012feasibility}\\ \hline
    Prefix/suffix            & 2-letter prefixes/suffixes divided by total number of prefixes/suffixes   & 676               & \cite{madigan2005author}\\ \hline
    Punctuation character   & Fraction of characters that are a known punctuation character             & 11                & \cite{madigan2005author} \cite{narayanan2012feasibility}\\ \hline
    Sentence length          & 1 divided by the average sentence length                                  & 1                 & \cite{de2001mining}\\ \hline
    Single occurring word    & Words that occur only once divided by all distinct words                  & 1                 & \cite{madigan2005author} \cite{narayanan2012feasibility}\\ \hline
    Upper case           & Words that are fully in upper case divided by all words                   & 1                 & original\\ \hline
    Word frequency           & 1 divided by the average number of occurrences per word                   & 1                 & \cite{madigan2005author} \cite{narayanan2012feasibility}\\ \hline
    Word length              & 1 divided by the average word length                                      & 1                 & \cite{argamon2003style} \cite{narayanan2012feasibility}\\
    \end{tabular}
    \end{center}
    \caption{Implemented features with descriptions and the count of individual features they contribute to the feature vector and papers they have previously been utilized in.}
    \label{tab:featureTable}
\end{table}


One of the features we utilized, the function word feature, might appear to clash with our desire to create a topic-independent algorithm, which does not use a bag of words model.
However, function words are words with little or no lexical meaning, mainly used to create the grammatical structure of a sentence.
Thus, they are actually topic-independent and the frequency of their usage has been successfully used to identify authors~\cite{mosteller1962applied}.
We also tried to achieve topic-independence for our list of common abbreviations, which we used in the same way.
The full list of function words and abbreviations we used for German and English can be found in Appendix~\ref{sec:app_function_words} and~\ref{sec:app_abbreviations}.

\end{savenotes}

After evaluating our features (see Section~\ref{sec:evaluation_clustering}) we decided to drop the \textit{prefix/suffix} and \textit{PoSTag} features and keep all others.



