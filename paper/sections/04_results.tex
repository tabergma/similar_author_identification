%!TEX root = ../paper.tex
\section{Evaluation and Results}
\label{sec:results}


To evaluate our two separate steps, we need a suitable data set with certain characteristics.
Firstly we want to measure the performance of our clustering into similar author groups.
Based on how this performance changes, when adding or removing features, we can also evaluate the value gain of features.
After we have the results from the clustering, we then use this mapping of blog posts on clusters to evaluate the prediction of new blog posts.
For example the cluster a blog post belongs to in the clustering result, is also the one we want to identify the blog post as, if it was not part of the clustering and classified afterwards.


\input{sections/04a_data_sources}


\subsection{Evaluation of Clustering}
\label{sec:evaluation_clustering}
To evaluate our \textit{k-means} algorithm we evaluated the feasibility and usefulness of our features (see Table~\ref{tab:featureTable}) by splitting them into 12 semantically related groups and then ran our \textit{k-means} clustering algorithm on each of the $2^{12} = 4,096$ possible permutations of active features using our small test data set (see Section~\ref{sec:data_sources}).
We set the number of desired clusters to six, allowing for the possibility of each author getting their own cluster.
We then used this scenario as a gold standard and calculated precision, recall, and F-measure wrt. an author as shown here:
\begin{equation}
	Precision = \frac{TP}{TP + FP} \\
	\label{eq:precision}
\end{equation}
\begin{equation}
	Recall = \frac{TP}{TP + FN} \\
	\label{eq:recall}
\end{equation}
\begin{equation}
	F-measure = \frac{2 \times Recall \times Precision}{Recall + Precision}
	\label{eq:fMeasure}
\end{equation}
For each author we find the cluster, which contains the highest number of documents from that author, the dominant cluster.
We assume that this is the best matching cluster for the author, so all documents from that author should belong to that cluster.
Based on this assumption, if author A has the dominant cluster C, we then define $TP$, $FP$ and $FN$ in the following way:
\begin{itemize}
	\item[$TP$:] Number of blog posts of author A, which are assigned to cluster C. \\
	\item[$FP$:] Number of blog posts from other authors, which are assigned to cluster C. \\
	\item[$FN$:] Number of blog posts of author A, which are not assigned to cluster C.
\end{itemize}

From these $4,096$ individual test runs we selected some meaningful data points to base our conclusions on.
One of these meaningful feature combinations was the one that gave the best overall F-measure of $61.62\%$.
This feature combination did not contain the \textit{prefix/suffix} and the \textit{part-of-speech tag} features.
This was surprising for us, because~\cite{madigan2005author} listed these as some of the features that performed best for them and because our own tests showed that having only these features active on their own provided us with better results than having only one of the other features turned on.
Looking further into the data, we found that turning off the \textit{prefix/suffix} or the \textit{PoSTag} feature while all other features remained active provided us with better results than having all features active at a time (see Table~\ref{tab:feature_evaluation_1}).

\begin{table}[ht!]
	\begin{center}
    \begin{tabular}{l|r|r}
	Feature name		& F-measure without & Difference to all features \\ \hline \hline
	All features active	& 0.4152833313 & $\pm$0.0 \\ \hline \hline
	Emoticon			& 0.3757936508 & +0.0438422688 \\ \hline
	Blank line			& 0.4074074074 & +0.0078759239 \\ \hline
	Post length			& 0.4196359196 & +0.0043525883 \\ \hline
	Word frequency		& 0.4130983959 & +0.0021849354 \\ \hline
	Upper case			& 0.4144100548 & +0.0008732765 \\ \hline
	Capital letter		& 0.4148865024 & +0.0003968289 \\ \hline
	Function word		& 0.4150935674 & +0.0001897639 \\ \hline
	Character frequency	& 0.4151904736 & +0.0000928577 \\ \hline
	Word length			& 0.4152373925 & +0.0000459388 \\ \hline
	Sentence length		& 0.4152833313 & $\pm$0.0 	   \\ \hline
	Part-of-speech tag				& 0.4196359196 & -0.0043525883 \\ \hline
	Prefix/suffix		& 0.4712048408 & -0.0559215095 \\
    \end{tabular}
    \end{center}
	\caption{The F-measure having all features but one active and the difference to the F-measure having all features active. Having the \textit{part-of-speech tag} feature or the \textit{prefix/suffix} feature active, decreases the overall F-measure.}
	\label{tab:feature_evaluation_1}
\end{table}

We concluded that the \textit{prefix/suffix} and \textit{part-of-speech tag} features provided merely decent (but not great) results on their own.
Due to their high number of individual features, we can use them alone quite well to distinguish blog posts, however the overall quality is not as good as other single features combined.
Features with only one individual value might provide a clearer separation but only in a single dimension.
This means that they are not good for creating a larger number of clusters, which is what would have been required for a good performance in our test with only one active feature.
However, due to the clear separation they provide, adding more small features (and thus dimensions) drastically improves the test results.
If we then add in the high dimensional but only mediocre features, such as \textit{prefix/suffix} and \textit{part-of-speech tag} as well, the test results become worse, because the positive impact of the low dimensional features is lessened by the sheer amount of dimensions added to the feature vector.
Thus, their good performance is being overshadowed by the high dimensional features' mediocrity.


Ultimately, we decided not to use the \textit{prefix/suffix} and \textit{part-of-speech tag} features, since they both lowered performance significantly.
Dropping them both meant that the number of features we had in a vector was now only a fraction of the previous amount, which gave our small but well-performing features much more impact.


The resulting F-measure for our \textit{k-means} algorithm when having all but the \textit{prefix/suffix} and \textit{part-of-speech tag} feature enabled is $61.64\%$.
When looking at the individual blog posts' classification, we noticed that the blog posts of two authors would mostly be sorted into the same cluster due to their feature vectors being quite similar.
We concluded that these two authors actually had a similar writing style and that this meant that the F-measure could not become much better due to our gold standard assuming one cluster per author.


\subsection{Evaluation of Classification}
\label{sec:evaluation_classification}

For evaluating our classification methods we used ten-fold cross validation~\cite{kohavi1995study}.
We divide our test data set into ten sets, each set containing nine blog posts.
Then we train our classification algorithms on nine of these ten sets and use the last set for testing.
This process is repeated ten times, so each one of these sets was a test set once.
Each time the precision is calculated and at the end the overall average precision is determined.


Training the \textit{support vector machine} means that a new model based on the training data set is created.
This model is then used for determining a cluster for each blog posts in the test set.
The \textit{k-nearest neighbor} algorithm uses the training data set as known data points (see Section~\ref{sec:k_nearest_neighbor}).


After finding the optimal parameters for both methods, we achieved $97.77\%$ precision for \textit{k-nearest neighbor} and $98.89\%$ precision for our \textit{support vector machine} in the ten-fold cross validation.


Thus, our result of $\sim60\%$ F-measure for clustering appears to be a lot better than those from related work in this field.
This is mainly due to the different problem domain though:
We merely try to find similar writing style groups, while most of the related work tried to identify the exact author to a document.
Since this problem is harder than ours, simply because the solution space is a lot larger, results that appear worse at first glance (such as the $20\%$ accuracy of Narayanan et al.~\cite{narayanan2012feasibility}) are to be expected.
On the other hand, Koppel et al.~\cite{koppel2003automatically}, who try to correctly identify an author's gender by their writing style, achieve a precision of over $80\%$ since this problem is smaller than even our six assumed writing styles.
The fact that our results lie somewhere between these two other approaches to (respectively) more and less difficult problems is thus exactly what was to be expected.