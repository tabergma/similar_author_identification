%!TEX root = ../paper.tex
\section{Evaluation and Results}
\label{sec:results}


There are two separate steps we want to evaluate.
Firstly we want to measure the performance of our clustering into similar author groups.
Based on how this performance changes, when adding or removing features, we can also evaluate the value gain of features.
After we have the results from the clustering, we then use this mapping of blog posts to a cluster to evaluate the prediction of new blog posts.
%TODO: Der Satz ist ziemlich kompliziert geschrieben!
For example the cluster a blog post belongs to in the clustering result, is also the one we want to identify the blog post as, if it was not part of the clustering and classified afterwards.


\subsection{Evaluation of Clustering}
\label{sec:evaluation_clustering}
To evaluate our k-means algorithm we evaluated the feasibility and usefulness of our features (see Table~\ref{tab:featureTable}) by splitting them into 12 semantically related groups and then ran our k-means clustering algorithm on each of the $2^{12} = 4096$ possible permutations of active features using our small test data set (see Section~\ref{sec:data_sources}).
We set the number of desired clusters to six, allowing for the possibility of each author getting their own cluster.
We then used this scenario as a gold standard and calculated precision, recall and f-measure wrt. an author as shown here:
\begin{equation}
	Precision = \frac{TP}{TP + FP} \\
	\label{eq:precision}
\end{equation}
\begin{equation}
	Recall = \frac{TP}{TP + FN} \\
	\label{eq:recall}
\end{equation}
\begin{equation}
	F-Measure = \frac{2 \times Recall \times Precision}{Recall + Precision}
	\label{eq:fMeasure}
\end{equation}
For each author we find the cluster, which contains the highest number of documents from that author, the dominant cluster.
We assume that this is the best matching cluster for the author, so all documents from that author should belong to that cluster.
Based on this assumption, if author A has the dominant cluster C, we then define $TP$, $FP$ and $FN$ in the following way:
\begin{itemize}
	\item[$TP$:] Number of blog posts of author A, which are assigned to cluster C. \\
	\item[$FP$:] Number of blog posts from other authors, which are assigned to cluster C. \\
	\item[$FN$:] Number of blog posts of author A, which are not assigned to cluster C.
\end{itemize}

From these 4096 individual test runs we selected some meaningful data points to base our conclusions on.
One of these meaningful feature combinations was the one that gave the best overall f-measure of $61.62\%$.
This feature combination did not contain the prefix/suffix and the PoSTag features.
This was surprising for us, because~\cite{madigan2005author} listed these as some of the features that performed best for them and because our own tests showed that only having these features active on their own provided us with better results than only having one of the other features turned on.
Looking further into the data, we found that turning off the prefix/suffix or the PoSTag feature while all other features remained active provided us with better results than having all features active at a time (see Table~\ref{tab:feature_evaluation_1}).

\begin{table}[h]
	\begin{center}
    \begin{tabular}{l|r|r}
	Feature name		& F-Measure without & Difference to all features \\ \hline \hline
	All features active	& 0.4152833313 & $\pm$0.0 \\ \hline \hline
	Emoticon			& 0.3757936508 & +0.0438422688 \\ \hline
	BlankLine			& 0.4074074074 & +0.0078759239 \\ \hline
	PostLength			& 0.4196359196 & +0.0043525883 \\ \hline
	WordFrequency		& 0.4130983959 & +0.0021849354 \\ \hline
	UpperCase			& 0.4144100548 & +0.0008732765 \\ \hline
	CapitalLetter		& 0.4148865024 & +0.0003968289 \\ \hline
	FunctionWord		& 0.4150935674 & +0.0001897639 \\ \hline
	CharacterFrequency	& 0.4151904736 & +0.0000928577 \\ \hline
	WordLength			& 0.4152373925 & +0.0000459388 \\ \hline
	SentenceLength		& 0.4152833313 & $\pm$0.0 	   \\ \hline
	PoSTag				& 0.4196359196 & -0.0043525883 \\ \hline
	Prefix/Suffix		& 0.4712048408 & -0.0559215095 \\
    \end{tabular}
    \end{center}
	\caption{The f-measure having all features but one active and the difference to the f-measure having all features active. Having the PoSTag feature or the prefix/suffix feature active, decreases the overall f-measure.}
	\label{tab:feature_evaluation_1}
\end{table}

We concluded that the prefix/suffix and PoSTag features provided merely decent (but not great) results on their own.
Due to their high number of features, they already have multiple dimensions by which we can distinguish their vectors.
%TODO: Nicht gut verst√§ndlich
Features with only one individual value might provide a clearer separation but only in a single dimension.
This means that they are not good for creating a larger number of clusters, which is what would have been required for a good performance in our test with only one active feature.
However, due to the clear separation they provide, adding more small features (and thus dimensions) drastically improves the test results.
If we then add in the high dimensional but only mediocre features such as prefix/suffix and PosTag as well, the test results become worse because the positive impact of the low dimensional features is lessened by the sheer amount of dimensions added to the feature vector.
Thus, their good performance is being overshadowed by the high dimensional features' mediocrity.


Ultimately, we decided not to use the prefix/suffix and PoSTag features, since they both lowered performance significantly.
Dropping them both meant that the number of features we had in a vector was now only a fraction of the previous amount, which gave our small but well-performing features much more impact.


The resulting f-measure for our k-means algorithm when having all but the prefix/suffix and PoSTag feature enabled is $61.64\%$.
When looking at the individual blog posts' classification, we noticed that the blog posts of two authors would mostly be sorted into the same cluster due to their feature vectors being quite similar.
We concluded that these two authors actually had a similar writing style and that this meant that the f-measure could not become much better due to our gold standard assuming one cluster per author.


\subsection{Evaluation of Classification}
\label{sec:evaluation_classification}

For evaluating our classification methods we used ten-fold cross validation~\cite{kohavi1995study}.
We divide our test data set into ten sets, each set containing nine blog posts.
Then we train our classification algorithms on nine of these ten sets and use the last set for testing.
This process is repeated ten times, so each one of these sets was a test set once.
Each time the precision is calculated and at the end the overall average precision is determined.


Training the support vector machine means, that a new model based on the training data set is created.
This model is then used for determining a cluster for each blog posts in the test set.
The k-nearest neighbor algorithm uses the training data set as known data points (see Section~\ref{sec:k_nearest_neighbor}).


After finding the optimal parameters for both methods, we achieved $97.77\%$ precision for k-nearest neighbor and $98.89\%$ precision for our support vector machine in the ten-fold cross validation.


Thus, our result of $\sim60\%$ f-measure for clustering appears to be a lot better than those from related work in this field.
This is mainly due to the different problem domain though:
We merely try to find similar writing style groups, while most of the related work tried to identify the exact author to a document.
Since this problem is harder than ours, simply because the solution space is a lot larger, results that appear worse at first glance (such as the $20\%$ accuracy of Narayanan et al.~\cite{narayanan2012feasibility}) are to be expected.
On the other hand, Koppel et al.~\cite{koppel2003automatically}, who try to correctly identify an author's gender by their writing style, achieve a precision of over $80\%$ since this problem is smaller than even our six assumed writing styles.
The fact that our results lie somewhere between these two other approaches to (respectively) more and less difficult problems is thus exactly what was to be expected.