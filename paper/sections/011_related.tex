%!TEX root = ../paper.tex

\section{Related Work}
\label{sec:related}


% topic independence, small number of documents
The field of author identification was already captured by De Vel et al.~\cite{de2001mining}.
Their work is based on only a small number of emails, written by three authors and spread over three topics: movies, food, and travel.
To allow author discrimination while covering multiple topics, the used features have to be topic-independent.
The paper furthermore states, based on~\cite{corney2001identifying}, that about 20 documents per author should be sufficient for a satisfying categorization performance.
De Vel et al. reach an average precision of $92.5\%$ for their author classification, when using both their structural features and style markers.
Compared to our approach, De Vel et al. focus on the email field, whereas we want to find similar authors in the blogosphere domain.
The presented features are topic-independent, which is also a goal we want to achieve.
Nevertheless, emails have different characteristics than blog posts, so some of the characteristic features may not work for our domain, blog posts.
Also, we want to classify authors based on a much larger data set.


\cite{madigan2005author} focuses on a larger scale with 27,000 documents and uses a higher number of more diverse features.
The results in this work contain the conclusion that features, such as \textit{bag of words} do work well for single topic author discrimination.
However for the topic independent classification other features, e.g., features based on function words, prefixes, suffixes, and part-of-speech tags, deliver better results.
We chose those topic independent features as a basis for our feature selection.


One of the few publications, which deals with a number of documents close to our desired data set size is~\cite{narayanan2012feasibility}.
The authors used 2.4 million blog posts from about 100,000 blogs with roughly the same number of authors.
This work shows how techniques from prior work do not scale well and which new techniques were used instead.
The machine learning approaches used were \textit{k-nearest neighbor}, \textit{naive Bayes}, \textit{support vector machine}, and \textit{regularized least squares classification}.
Interesting new features include the frequency of special characters, such as $*$, $=$, $+$, $[$ or $]$ and the word shape, which is the frequency of words with different combinations of upper and lower case letters.
Because Narayanan et al. have a similar setup, we tried some of their features.
However, because Narayanan et al. focus on finding the exact author and identify the correct one with about $20\%$ accuracy, we are focusing on finding similar author groups.


Identifying certain traits of an author, which could be used to distinguish between different authors is also a popular research area.
The authors of~\cite{koppel2003automatically} have attempted to discern an author's gender in fictional and non-fictional works.
Their approach uses function words and part-of-speech tags as singles, tuples, and triples.
After cross validating their machine learning algorithm, they managed to reach over $80\%$ precision for the author's gender.
However, Koppel et al. classified books and articles, which are generally much longer than blog posts.
