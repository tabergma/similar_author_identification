\subsection{Clustering}
\label{sec:impl_clustering}

% TODO

\subsubsection{K-Means Algorithm}
\label{sec:impl_k-means}
For our implementation we used the k-means algorithm offered by Mahout\footnote{\url{https://mahout.apache.org/}}, which can be executed efficiently among a cluster with a large amount of data~\cite{esteves2011k}.


K-means creates $k$ clusters from $n$ vectors by randomly selecting $k$ points from the vector space as cluster centroids.
Then, each vector or point is assigned to its nearest cluster centroid according to its Euclidean distance to those centroids.
Afterwards, the cluster centroids are repositioned to the center of all vectors/points which were assigned to them.
These last two steps are repeated until the cluster centroids converge.
The resulting assignment of the vectors (and thus their corresponding blog posts) to the clusters is the same as the final assignment performed by the k-means algorithm.


Using our small data set we set $k$ to the number of authors ($6$).
Under the assumption, that an author has an individual writing style and that this writing style is not changing over time, an author should get its own cluster only containing blog posts written by him.
In this way, we could evaluate our k-means algorithm (see Section~\ref{sec:evaluation_clustering}).


\subsubsection{Cluster Labelling}
\label{sec:impl_cluster_labeling}
For the cluster labelling implementation we used the method 3, presented in Section~\ref{sec:cluster_labeling}.
This method assigns $n$ label, based on the most significant $n$ features, to a cluster.
In our final implementation each cluser obtains five different labels, so $n$ was set to $5$.